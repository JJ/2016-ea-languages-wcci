\documentclass[conference,twocolumns]{IEEEtran}

\usepackage{cite}
\usepackage{array}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{subfigure}

\begin{document}
%\SweaveOpts{concordance=TRUE}

<<setup, cache=FALSE,echo=FALSE>>=
library("ggplot2")
library(RColorBrewer)
measures.bf <- read.csv('measures-bitflip.csv')
measures.xo <- read.csv('measures-xover.csv')
measures.mo <- read.csv('measures-maxones.csv',header=TRUE,sep=",",dec=".")
@


\title{A comparison of implementations of basic evolutionary
 algorithm  operations in different languages} 

%PABLO: in conference mode this is how to create the authors
\author{\IEEEauthorblockN{Juan-Julián Merelo-Guervós,Israel
    Blancas-Álvarez, Pedro A. Castillo, Gustavo Romero }
\IEEEauthorblockA{CITIC and Computer Architecture and Technology
  Department\\
University of Granada (Spain)\\
Email: (jmerelo,iblancasa,pacv,gustavo)@ugr.es}
\and
\IEEEauthorblockN{Victor M. Rivas}
\IEEEauthorblockA{Department of Computer Sciences\\
University of Jaén (Spain)\\
Email: vrivas@ujaen.es}
\and
\IEEEauthorblockN{Mario García-Valdez, Amaury Hernández-Águila}
\IEEEauthorblockA{Tijuana Institute of Technology (Mexico)\\
Email: {amherag,mario}@tectijuana.edu.mx}
\and
\IEEEauthorblockN{Mario Román}
\IEEEauthorblockA{University of Granada (Spain)\\
Email: mromang08@correo.ugr.es}

}

\maketitle

\begin{abstract}
  It is not usual practice in the evolutionary algorithms area to benchmark
different operations in order to choose the best language for a single
or multilanguage implementation. Researchers rely instead on common practice or
frameworks using mainstream languages. 
That is why it is usual practice to
choose compiled languages (namely Java or C/C++) when implementing
evolutionary algorithms, without considering other languages or
rejecting them outright on the basis of performance. Since there is a
myriad of languages nowadays, we considered it an interesting challenge
to measure their speed when performing frequent operations in
evolutionary algorithms. In this paper we have tested three basic
evolutionary algorithm operations over binary chromosomes: bitflip
mutation, crossover and the OneMax fitness function.  As a performance
measure, the speed for both popular and not so popular computer
languages have been used.  In general, the results confirm that
compiled languages scale and perform better, but also in some cases
have a behaviour that is independent of the size of the
chromosome. Additionally, results show that other languages, such as
Go (compiled) or Python (interpreted) are fast enough for most
purposes.  Besides, these experiments show which of these operations
are, in fact, the best for choosing an implementation language based
on its performance.
\end{abstract}

\begin{IEEEkeywords}
Per\-formance optimization, bench\-mark, implementation of
evolutionary algorithms, onemax, programming languages.
\end{IEEEkeywords}

\section{Introduction}

Most released evolutionary algorithms (EA) implementations, and also most
experiments published, use mainstream compiled languages such as C/C++
or Java. In general, researchers consider that compilation makes
running the experiment faster and thus make, in general, the algorithm
more efficient. On the other hand,
the choice of a particular language might be constrained by the
application at hand; some optimization frameworks might need to link
to resources such as embedded
or web based systems, forcing them to rely on a particular language or
a set of languages. Past experience or simply the know-how in teams will
also make them to choose languages outside the ones mentioned above. But,
running speed is not everything. In many cases, 
ease of integration with existing or legacy tools, coding speed or
availability of parallel or distributed frameworks are much more
important than how fast a single CPU program runs. In some
cases, mainly when the size of the problem or the running time of the
algorithm call for the maximum running speed available, it is
interesting at least to know which languages can be used to obtain the
best performance, or at least which one is fastest among the set of
languages the research team is able to handle. 

That is why when efficiency matters, as said in
 \cite{anderson2010efficiency} and, in general, if we restrict
 performance to {\em speed of the compiled/interpreted application
   when run} it might be the case that some
languages are faster than others, as evidenced by benchmarks such as
\cite{prechelt2000empirical,fulghamcomputer}. Taken in general or even
restricting it to some particular set of problems such as floating
point computation, some compiled languages tend to be faster than
interpreted languages as proved in the papers mentioned above.

However, in the same spirit of the {\em No Free Lunch} theorem
\cite{Wolpert-1997-NFL} we could consider there is a {\em no fast lunch}
theorem for the implementation of evolutionary optimization problems, in the
sense that, while there are particular languages that might be the
fastest for particular problem sizes and specially fitness functions,
these two constraints cannot be disregarded, and,
specially, for non-trivial problem sizes and limiting ourselves to the
realm of EA operators, some interpreted and
unpopular languages such as {\tt awk} can be the fastest option
available for evolving particular data structures such as regular
expressions \cite{awk}. Besides, benchmarks should include a wide variety
of sizes, because the time needed to perform particular operations
does not always depends linearly on size, as performance is also affected
by technical details for instance the implementation of loops and
memory management. In fact, if running speed is the only issue,
languages such as Fortran, and implementations on GPUs would be the
way to go for all researchers. However, as that is not the case, we can
conclude that we are looking for languages that are, if not the
fastest, fast enough, and also within the set of languages that are
used nowadays. Besides, it might be the case that some emerging
language whose popularity in the area of EAs is
not too big, is, in fact, faster than the well known C++ or Java. 

For the purposes of this paper, we will try to be more extensive on
the number of languages tested than on the number of operations,
choosing from a varied set of languages placed with different paradigms. 
Since in
general, EAs use fitness functions that can have
any size and shape, it is not easy to cover them all and further
characterization might be needed. We are going to focus on
combinatorial optimization problems, which are usually represented
using bit strings or other structures similar to them and the two most characteristics operators: mutation
and crossover. Besides, a fitness function usually employed as a
baseline for EAs will also be used: OneMax. 

We have to admit that, in general, programs do not spend the majority
of the time applying them; this place in the ranking rather goes to
selection operators and other higher-level, population-handling ones, as well as the
fitness function which, in most cases, occupies most of the  % "the majority of"  => "most of"
% Do changes, do not suggest them if you are sure! - JJ
running time. However, these functions are well covered by usual
general purpose
benchmarks so you can usually rely on any of them for choosing a
language to implement that part of your EA. This makes the
scope or interest of this paper certainly restricted to the set of
problems in which classical bit-wise operations are performed and
where fitness does not take most of the time: Testing new operators or
implementing parallel or other kind of algorithms on functions such as
HIFF, OneMax or Royal Road. However, the objective of this paper is
not to optimize the total running time of EAs by
choosing the best language or data structure but rather to prove that
there are many interesting options with a good performance that should
be considered by EA practitioners. We were  % this acronym should be defined before, and used along the rest of the text
interested also in checking out the design decisions that the
different languages forced us to do and what was the impact on overall
performance. 

Let us remark that our intention is not so much to choose the winner of the
competition of fastest language for EAs as much as
to check the variety of speeds available and to know what order of
magnitude these differences are. This work can be used to aid in
making the decision of a language for implementing an evolutionary
algorithm, or at least to justify the choice of non-mainstream
languages such as Python, Lua, Scala, JavaScript or Go, which, in fact, do
reach interesting performance marks. Also having a repository with the
implementation of basic EA operators in different languages could be useful
to researchers or students that are learning to program, or evaluating different
technologies. 

Coming up next in Section \ref{sec:soa}, we will present the state of the art of the analysis
of EA implementations. Next we will present in
Section \ref{sec:exp} 
the tests we have used for this paper and its rationale along with the
languages we have chosen for carrying them out. Finally, in Section
\ref{sec:res} we 
will present the results of measuring the performance of eleven
languages running some widely used EA operators and functions 
mutation, crossover and OneMax. Finally, we will draw the conclusions
and present future lines of work.

\section{State of the art}
\label{sec:soa}

As a matter of fact, the examination of the running time of an evolutionary
algorithm has received some attention from early on, with programming
environments reviewed already in 1994 \cite{jose1994genetic}. This initial
%Pablo: Paloma told me that semicolons are evil
%they probably are... - JJ
survey focused on implementation aspects, but shows that most of them
used C or C++; Java was not even an option back then. Implementation
%Pablo: well, here the semicolon is not that bad :)
matters \cite{DBLP:conf/iwann/MereloRACML11,nesmachnow2011time}, which implies that
paying attention to the particular way an algorithm is implemented
might result in speed improvements that outclass those achieved by
using the {\em a priori} fastest language available. Some authors have
even proved that careful and expert coding, and choosing the right tools \cite{ae09,1515367} in
interpreted languages can make them as fast, or even faster, than
compiled languages such as Java.

However, most papers devoted to the implementation of evolutionary
algorithms in languages other than C or Java try to prove that, for
the particular type of problems used in scientific computing in general, the running speed is not as important
as coding speed or even learning speed, since most scientific programs
are, in fact, run only a few times, each one of them lasting all of a
few seconds, while a lot of time is spent on coding
them. That is why expressive languages such as Perl, JavaScript or
Python are, in many cases, superior to these fast-to-run
languages \cite{ae09}. %Pablo: citation to the specific paper that says this?
%Citing our own paper here - JJ

Even so, and when speed matters, the benchmarks performed in the papers
mentioned above were restricted to
particular problem sizes and to very specific languages. They also
test a single language for the whole EA. However,
it might happen that, since different operations are involved, the
place in the ranking for different languages is different depending on
the operation and on the size. This 
can be of special interest in
environments such as the Kappa \cite{KappaArch}, microservices \cite{namiot2014micro} or
service-oriented frameworks 
\cite{garcia2010distributed,DBLP:journals/soco/Garcia-SanchezGCAG13} 
where we could envision that different parts of an evolutionary algorithm might be written in different
languages. An advantage of these loosely connected systems is that they might
switch the language used for a particular operation if they encounter
performance issues, as opposed to monolithic architectures written
in a single language.

This is, as a matter of fact, the state of the art such as we have found them. 
That is why we think it is important to take real measures so that decisions 
on implementation are based on facts strictly related to EAs, instead
of relying on common lore. 

Next we will explain the operations used for the benchmark and how
they have been tested.

\section{Experimental setup}
\label{sec:exp}

Several operations: bitflip mutation, crossover and count-ones or onemax, have been chosen for testing different
languages and also data representations. This is not the complete set
of EA operations or the most time-consuming \cite{nesmachnow2011time}, but they are representative
because they have to be implemented in all evolutionary algorithms,
the last one as a baseline test. In general, the fitness function in
non-trivial or academic problems will be the speed bottleneck,
followed in most cases by reproduction-related functions: chromosome ranking, for
instance. However, there are many different ways to implement this and
they cannot be covered in this paper, so we will stick to the most
common and usual functions, which we will describe next. 

The first one, {\em bitflip mutation}, changes a single bit chosen
randomly from the random function included in the standard library on
a binary string. However, mutation is an operation that is performed
quite frequently and sometimes once for every newly generated
individual; it is also quintessential to 
the algorithm itself and one of the pillars of the canonical genetic
algorithm, so it allows the comparison of the different 
languages in the proper context. 

The other function chosen, crossover, is also part of that canonical EA, and MaxOnes or Count-Ones or OneMax is a fitness
function frequently used as baseline for comparison of evolutionary
algorithms. Its implementation is trickier than it seems a priori, and
making the fastest implementation in a particular language might be
more difficult than it a priori seems. 


In this section we will outline first
the specifics of the implementation and expand the rationale behind
choosing them for this paper in
subsection \ref{ss:operators}. Then we will proceed to outline the different
data structures that have been used here in subsection \ref{ss:ds} to
finally present the different languages that have been tested and the
peculiarities of its implementation \ref{ss:lang}.

\subsection{Functions and operators included in the benchmark}
\label{ss:operators}

Essentially, mutation is performed by \begin{enumerate}

\item Generating a random integer from 0 to the length of the chromosome.
\item Choosing the bit in that position and flipping it
\item Building a chromosome with the value of that bit changed or
  returning the chromosome with that bit changed.

\end{enumerate}

These operations deal mostly with random number generation and then
list, string or vector processing. In general, the time needed for copying and creating
strings could depend on the length, but its implementation might vary
from one language to another and in some cases copying is not even
used, working directly with the changed chromosome. 

The next operation, {\em two-point crossover} is performed as follows:
\begin{itemize}
  \item Generating two random integers with a range from 0 to the
    length of the chromosomes. It might be necessary to check that the
    second is greater than the first.
  \item Building two new chromosomes including the original from
    position 0 to the first point, interchanged bits from the first
    point to the second, and the original ones from the second
    position to the end of the strings. 
\end{itemize}

A priori this operation seems quite similar to the first one. However,
it necessarily involves copying of strings, an operation that will scale in a
different way than simply running over a string or accessing one of
its components and modifying one
bit.

Finally, {\em OneMax} follows this procedure
\begin{itemize}
\item Generate a random string.
\item Loop over the string.
\item If the bit is set to one, add one to a counter.
\item Return the value counter.
\end{itemize}

Despite its apparent simplicity, counting the number of ones in a
string is an extremely complicated operation, which is in fact used by
human resources teams to examine the prowess of candidates in the
creation of algorithms and in the knowledge of a language. The
straightforward way of carrying it out is using a loop that looks, one
by one, at the bits in the string and adds 1 to the counter if they
are set to this value; or simply adds the value of the bit to the counter. However,
in most cases using {\em external} loops might not be the fastest way. At any rate, this
fitness function is quite similar to others that decode the bits of a
binary chromosome and, even if it is quite clearly not the only
existent fitness function, it is one that is widely used in
EAs and whose speed can be applied to other
similar functions.

In most cases, the function is implemented as a loop. Thus, we should
expect that the time needed would 
grow linearly with the chromosome size. We will check whether this is
true or not for the different languages in the next section. 

Next we will have a look at the data structures used in the different
languages. 


\subsection{Available data structures}
\label{ss:ds}

Chromosomes in EAs can be represented in several different ways: an
array or vector of Boolean values, or any other scalar value that can
be assimilated to it, or as a bitstring using generally ``1'' for true
values or ``0'' for false values. Different data structures will have
an impact on the result, since the operations that are applied to them
are, in many cases, completely different and thus the underlying
implementation is more or less efficient. Besides, languages use
different native data structures to represent this information. In
general, it can be divided into three different fields:\begin{itemize}
\item {\em Strings}: representing a set bit by 1 and unset by 0, it is
  a data structure present in all languages and simple to use in
  most.
\item {\em Vector of Boolean values}: not all languages have a
  specific primitive type for the Boolean false and true values; for
  those who have, sometimes they have specific implementations that
  make this data structure the most efficient.
  \item {\em Vectors} of integers or even characters can also be used
    in the case of the language lacking a specific data structure for
    bits or Beans. 
\item {\em Bitsets}: since the internal representation of any data
  structure is composed, at a low level, by a set of bits,  you can
  simply use bits packed 
  into bytes for representing chromosomes, with 32 bits packed in a
  single 4 byte data structure and bigger number of bytes used as
  needed . Memory-wise the most efficient,
  without low-level access operators it can indeed be the slowest, and
  in any case not too efficient for decoding to function parameters.
\end{itemize}

Besides, many languages, including functional ones, differentiate
between Mutable and Constant data structures, with different internal
representations assigned to every one of them, and extensive
optimizations used in Immutable or constant data structures. Immutable
vectors were introduced by Clojure in 2006, and they have extended to
other functional languages such as Scala. In the
case they are available, we will use them, in some cases comparing it
with Mutable data structures. In the special case of the bitflip 
operator a Mutable data structure can be more efficient, because many
of the elements are modified in a loop.


The memory and speed efficiency of these data structures is different,
and it is advisable for anyone implementing an EA to check all
possible data structures before committing, out of inertia, to the
easiest one. Once again, implementation matters
\cite{DBLP:conf/iwann/MereloRACML11,nesmachnow2011time}, and
differences in EA performance for different data structures can be,
indeed, quite big. 

Not all data structures, however, are easily available for every
language, or easy to deal with. We will check in the next subsection
which ones have been used in every language. 


\subsection{Languages tested}
\label{ss:lang}

In this paper, 16 languages have been chosen for performing all
benchmarks; additionally, another language, Rust, has been tested for
one of them. The primary reason for choosing these languages was the
availability of open source implementations for the authors, but also
we have tried to be inclusive in by considering languages that 
represent different philosophies in language design as well as languages
traditionally used in the implementation of EAs
together with others that are not so popular. These 
languages are presented in the next two subsections and listed in
Table \ref{tab:files}. We will examine in turn compiled languages
(next) and interpreted languages in the next subsubsection.

\begin{table*}[h!tb]
    \centering
    \begin{tabular}{l|c|l|l|l}
      \hline
      Language & Version & URL & Data structures & Type\\
      \hline
      C & 4.8.2 & \url{http://git.io/v8T57} & char vector & Compiled \\
      C++ & 4.8.4 & \url{http://git.io/v8T57} & bitset  & Compiled\\
      C\# & mono 4.2  & \url{https://git.io/vzHDI} & Bit Vector  & Compiled\\
      Clojure & 1.8.0 & \url{https://git.io/vzHDe} & Persistent Vector &  Compiled \\ 
      Common Lisp & 0.13.7 & \url{https://git.io/vzHyR} & Simple Bit Vector &  Compiled\\ 
      Go & go1.2.1 & \url{http://git.io/vBSYp} & Bit Vector & Compiled\\
      Haskell & ghc 7.10.3 & \url{https://git.io/vzHMw} & Mutable Vector & Compiled \\ 
      Java & 1.8.0\_66 & \url{http://git.io/v8TdR} & Bitset & Compiled\\
      JavaScript & node.js 5.0.0 & \url{http://git.io/vBSYd} & String & Interpreted\\
      Julia & 0.2.1 & \url{http://git.io/vBSOe} & Bit Vector & Interpreted \\
      Lua & 5.2.3 & \url{http://git.io/vBSY7} & String & Interpreted\\
      Octave & 3.8.1 & \url{http://git.io/v8T57} & BitVector & Interpreted \\ 
      PHP & 5.5.9 & \url{http://git.io/v8k9g} & String & Interpreted\\ % vrivas, 5-nov-2015
      Perl & v5.20.0 & \url{http://git.io/bperl} & String, Bit Vector & Interpreted \\
      Python & 2.7.3 & \url{http://git.io/vBSYb} & String & Interpreted\\
      Rust &  1.4.0  & \url{https://github.com/JJ/RustEO} & BitVector  & Compiled \\ 
      Scala & 2.11.7 & \url{http://git.io/vBSYH} & String, Bit Vector & Compiled \\
      \hline
      \end{tabular}
      \vspace{5mm}
      \caption{Languages, versions, URLs, data structure and type of
        language used to carry out the
        benchmarks. No special flags were used for the interpreter or
        compiler. \label{tab:files}}
    \end{table*}
%

    \subsubsection{Compiled languages}
    \label{ss:compiled}


    These languages run in different ways, but there are
    representatives of both compiled (native and to bytecode) and
    interpreted. 
    Compiled languages are represented by Haskell, Rust, Scala, Java,
    Common Lisp, Clojure, Go, C, C\# and 
    C++. Scala, Clojure and Java use Java bytecode and run in the Java
    Virtual Machine.  C\# runs in the Common Language Runtime. Go,
    Common Lisp, C and C++ compile to a binary that runs 
    natively on the target operating system. Also JavaScript is
    compiled to native machine code when it runs in the V8 JavaScript
    Engine used by both Node.js and Google Chrome web browser.  This
    list of languages is rather comprehensive, including the most
    popular compiled languages in scientific computing as well as three
    languages that are emerging in popularity: Go, Scala and Clojure. 

    
Scala \cite{odersky2004overview} is a strongly-typed
functional language that compiles to a Java Virtual Machine
bytecode. Scala is in many cases faster than Java
\cite{fulghamcomputer} due to its more efficient
implementation of type handling. In this paper, two different representations were
used in Scala: {\tt String} and {\tt Vector[Boolean]}. They both have
the same underlying type, {\tt IndexedSeq} and in fact the overloading
of operators allows us to use the same syntax independently of the
type. The benchmark is available under a GPL
license, at the URL shown in Table \ref{tab:files}. As far as we know,
there are no  EA frameworks published in
Scala; however, its increasing popularity within the programmer, and,
over all, data science community makes it quite likely to find one in the
near future.

C\# \cite{hejlsberg2003c} is a general-purpose, object-oriented programming language
designed for the Common Language Infrastructure, a virtual machine
designed as an alternative to the Java Virtual Machine.  In recent
versions C\# has added several major features to accommodate functional-style programming, 
and dynamic binding. For this work the C\# scripts were compiled to run in the Mono runtime, an 
implementation of the ECMA Common Language Infrastructure (CLI). This
is an open source version of the runtime, which allowed us to run it
in Linux.

Java is probably the most popular language within the EA community,
with several well established free software frameworks available, such
as JCLEC \cite{ventura2008jclec} or ECJ \cite{luke2006ecj}. It is a
multi-platform language, with compiled {\em bytecode} running in any
operating system with a native Java virtual machine (JVM). % todo:
                                        % include references. 

Clojure \cite{hickey2008clojure}  is a dialect of Lisp, 
and shares with Lisp the code-as-data philosophy and a powerful macro system and
Is compiled to run on the Java Virtual Machine. Clojure as a Java library consumer,  
can call EA libraries in Java and supports the dynamic implementation 
of Java interfaces and classes.

Common Lisp is also a dialect of Lisp, in this work we used the Steel Bank Common Lisp (SBCL) 
implementation \cite{rhodes2008sbcl}, a free high performance native compiler that has
been ported to multiple hardware architectures and operating systems. 

C is still one of the most popular languages, despite being free of
the paradigms that are so popular nowadays: object orientation or
concurrency. It is not so popular within the EA community, who usually
opt for its object-oriented version, C++, that has representatives
such as ParadisEO \cite{liefooghe2011software} or MALLBA
\cite{alba2002mallba}. 

Go \cite{pike2009go} was a language initially introduced by Google,
but that has dug into the niches that were dominated formerly by C. It
is also a compiled language designed for system programming, and is
concurrent. 

Haskell \cite{hudak1992report} is also purely functional language with
an efficient native compiler. Although it keeps being a niche
language, its popularity is increasing with applications such as
Pandoc and literary programming being its most popular cases. Since it
works functionally, we have included it in this paper to check it
against other functional languages. 

The Rust language \cite{hoarerust} is only tested in one function, and
used here only as another type of emerging language whose performance
would be interesting to measure. Rust is multi-paradigm and type-safe,
but supports also purely functional programming.  

These languages will be tested against a set of interpreted languages
described below. 

\subsubsection{Interpreted languages}

Interpreted languages are represented by Lua, PHP, Julia, Perl, Python
and JavaScript. These include three popular languages, Perl, Python
and JavaScript, Python being also the most popular in the scientific
computing environment, and then three other languages seldom seen in
the EC arena, but popular in their particular niches. 

Lua \cite{ierusalimschy1996lua} is a popular embedded language designed for easy
implementation, including a minimalist grammar so that its interpreter can be reduced to a few hundreds of Ks. Indeed, it can be lately found in places such as Game Engines.% todo
There are no known frameworks for EAs written in Lua, although an EA that runs in a Canon camera has been found in YouTube.

Perl is an interpreted language that has been used extensively for evolutionary
algorithms \cite{ae09,DBLP:conf/cec/GuervosMCCV13} with
satisfactory results, although its main emphasis is not on speed, but on freedom for the programmer to code using any style and paradigm. The module and tools ecosystem built around it make it extremely interesting for any kind of programming. Our implementation of Perl uses different open-source libraries and in some cases also different versions of the language. Perl produces new even minor versions every spring, with the current version being 5.22. Odd minor versions such as 5.23 are considered development versions, with features that are, later on, implemented on the stable versions of the language.

JavaScript is an interpreted language that follows an ECMA standard
and with many different implementations, including the different ones
embedded in all internet browsers. Node.js is one such implementation
of JavaScript with an asynchronous input/output loop and designed to
run standalone in servers or the command line. Node.js uses a the V8
JIT compiler to create native machine code when in reads the 
script, and has been used lately as part the NodEO
library \cite{DBLP:conf/gecco/GuervosVGES14} and the volunteer computing
framework NodIO \cite{DBLP:journals/corr/GuervosG15}, as well as other libraries focused on the browser \cite{rivas2014object}.

PHP is a language that is most commonly known for the creation of web sites. In fact, it is an interpreted and general-purpose language with emphasis on string processing. There is no known implementation of EAs in PHP.

Python is probably the most popular scripting language, head to head with JavaScript. It is directly supported by major corporations such as Google, and its different implementations (including one for the Java Virtual Machine, Jython) make it a very strong contender in the EA arena. In fact, several EA frameworks, such as DEAP \cite{fortin2012deap} use it.

GNU Octave \cite{eaton2005high} is a free interactive environment for numerical computation 
and is intended to be compatible with MATLAB. The Octave scripting language is executed using an interpreter and supports 
many common C standard library functions. 

Julia \cite{bezanson2012julia} is the last language tested. It is a
modern, interpreted and dynamic language used mainly for technical
computing. It cannot be called exactly popular, but we have included
it for completeness, mainly.

The interpreter versions and repositories for these languages are
shown in Table \ref{tab:files}.

\subsection{Implementation notes}

When available, open source implementations of the operators and OneMax were used.
In all cases except in Scala, implementation took less than one hour and was
inspired by the initial implementation made in Perl. Adequate data and control
structures were used for running the application, which applies
mutation to a single generated chromosome a hundred thousand
times. The length of the mutated string starts at 16 and is doubled
until reaching $2^{15}$, that is, 32768. This upper length was chosen to
have an ample range, but also so small as to be able to run the
benchmarks within one hour. Results are shown next. In some cases and
when the whole test took less than one hour, length was taken up to
$2^{16}$.

In most cases, and specially in the ones where no implementation was
readily available, we wrote small programs with very little overhead
that called the functions directly. That means that using classes,
function-call chains, and other artifacts, will add an overhead to the
benchmark; besides, this implies that the implementation is not
exactly the same for all languages. However, this inequality reflects
what would be available for anyone implementing an evolutionary
algorithm and, when we think it might have an influence on the final
result, we will note it. 

Every program used also provides native capabilities for measuring
time, using system calls to check the time before and after operations
were performed. This might result in a slightly different behavior,
but it is the best system available, so it is what we used. 

All programs produced the same output, a comma separated set of values
that includes the language and data structure used, operand length and
time in seconds. Results were eventually collated 
and are available in the same repository than this paper, together
with the code in several languages that was produced also specially
for this. 

\section{Results and analysis}
\label{sec:res}

We will examine first the performance for the bitflip operation, that is graphed in Figure \ref{fig:time}.
%
\begin{figure}[h!tb]
  \centering
<<results-bf, cache=FALSE,echo=FALSE>>=
colourCount = length(unique(measures.bf$languagerepresentation))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))
ggplot(measures.bf,aes(x=length,y=time,colour=factor(languagerepresentation)))+ geom_line() + geom_point() +  ggtitle("Evolutionary algorithm language benchmarks: Bitflip")+scale_y_log10()+scale_x_log10()+scale_color_manual(name='Language',values=getPalette(colourCount))
@
\caption{Plot of the time needed to perform 100K mutations in strings with
lengths increasing by a factor of two from 16 to $2^{15}$. Please note
that $x$ and $y$ both have a logarithmic scale.}
\label{fig:time}
\end{figure}
%
First, let us look at the {\em flat} lines, which represent those languages whose speed is
independent of the length. In many cases it corresponds to
implementations that do not perform any kind of copy, so that
exclusive fact  has nothing to do with speed. Even so, Java, CSharp,
Go, Scala show a remarkably fast behavior, almost one order of
magnitude faster than other languages. PHP is the fastest of
interpreted languages, with speed on a par with interpreted
languages and faster than C++. Clojure, on the other hand, is very slow.

Please note that, although the fastest in this case are always
compiled languages, PHP is faster than C++ and some languages such as
Python is faster than Scala. Perl is one of the slowest, but the {\em simple}
implementation beats Scala and Clojure for non-trivial lengths of the string. This
leads to the blurring the clear-cut distinction between {\em fast}
compiled languages and {\em slow} scripting languages. But please note
too that, for bigger sizes, the difference between the slowest and the
fastest can be several orders of magnitude, up to 5. The situation is
quite similar in Figures \ref{fig:time:xo} and \ref{fig:time:mo}. In
general, some compiled languages show higher performance, although in
some cases languages such as Node.js can achieve similar speeds for
some sizes. Scaling behavior is also similar, due to copy and other
linear operations like loops.

\begin{figure}[h!tb]
  \centering
<<results-xo, cache=FALSE,echo=FALSE>>=
colourCount = length(unique(measures.xo$languagerepresentation))
ggplot(measures.xo,aes(x=length,y=time,colour=factor(languagerepresentation)))+ geom_line() + geom_point() +  ggtitle("Evolutionary algorithm language benchmarks: Crossover")+scale_y_log10()+scale_x_log10()+scale_color_manual(name='Language',values=getPalette(colourCount))
@
\caption{Plot of time needed to perform 100K crossover operations in strings with
lengths increasing by a factor of two from 16 to $2^{15}$. Please note
that $x$ and $y$ both have a logarithmic scale.}
\label{fig:time:xo}
\end{figure}
%
\begin{figure}[h!tb]
  \centering
<<results-mo, cache=FALSE,echo=FALSE>>=
colourCount = length(unique(measures.mo$languagerepresentation))
ggplot(measures.mo,aes(x=length,y=time,colour=factor(languagerepresentation)))+ geom_line() + geom_point() +  ggtitle("Evolutionary algorithm language benchmarks: Onemax")+scale_y_log10()+scale_x_log10()+scale_color_manual(name='Language',values=getPalette(colourCount))
@
\caption{Plot of time needed to perform 100K OneMax evaluations in strings with
lengths increasing by a factor of two from 16 to $2^{15}$. Please note
that $x$ and $y$ both have a logarithmic scale.}
\label{fig:time:mo}
\end{figure}
%
\begin{figure}[h!tb]
  \centering
<<results-ratios,message=FALSE, cache=FALSE,echo=FALSE>>=
ratios <- read.csv('ratios.dat')
ggplot(ratios,aes(x=reorder(Language, -Ratio, FUN=median),Language,y=Ratio))+ geom_boxplot(notch=TRUE)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+scale_y_log10()+labs(x='Language')
@ 
\caption{Boxplot of scaled performance compared to baseline Julia. Please note
that $y$ has a logarithmic scale.}
\label{fig:ratios}
\end{figure}

The remarkable speed Java shows is consistent, so we should note here
that Java 
uses a built-in primitive, {\tt cardinality}, that measures the number
of bits of the set that are, effectively, set. We fail to understand
how the speed increases so much in the high end of the size range, but
it might be due to a change to a more efficient implementation on the
Java side. It is quite clear, however, that using this data structure
and associated operators makes Java stand out among the rest. This
speedup might not translate to other operations.

To have a general idea of performance and be able to compare across
benchmarks and sizes, we have used Julia as a baseline for comparison
and expressed all times as the ratio between the time needed for a
particular language and length and the speed for Julia. Julia has been
chosen because it is never the best or the worst, staying in the
middle. Ratios higher than 1 mean that the particular language+data
structure is faster than Julia, <1 the opposite. Please check the
boxplot in Figure \ref{fig:ratios}. The performance of Java and C\# is
quite similar, although the latter is more consistent. Clojure and
Haskell have a remarkable position in the ranking, better than Scala,
Go and C, which take the following positions. However, the fact that
Scala and Clojure are also at the other end of the scale with
different data structures indicate that it is important to know the
language well and that choosing a particular data structure might
result in better performance boosts than choosing a particular language.

\section{Conclusions}

In this paper we have measured the performance of an extensive
collection of languages in simple and common evolutionary algorithm
operations: mutation, crossover and onemax with the objective of
finding out which languages are faster at these operations and what
are the actual differences across languages, language types and data
structures. 

We can conclude that the language usually considered the fastest among
the practitioners,
Java, indeed holds that position. However, compiled languages need not
be much faster than interpreted languages with Perl, for instance,
having a speed which is very close to C and Clojure, a scripting
language that compiles to the Java Virtual Machine, being faster than
both. But in this case it is using a particular data structure, and
our tests also show that data structures and code layout have to be
chosen carefully with performance in mind. An incorrect choice and the
performance advantage of fast languages such as Scala will disappear. 

Among interpreted languages Perl, node and PHP are the fastest, with
languages such as Octave or Python having lower performance that
should be expected. Haskell, on the other hand, is included among the
fastest, but it should be noted that in a particular case we 
could not perform the test due to excessive time, so this advantage
should extend only to particular lengths. 

All these tests lead us to the conclusion that there is no type of language
that is superior to all others across all chromosome sizes, problems and data
structures, although if you use Java with BitSets or C\# you will be
close to the best or reach the best running speed. There is no
free lunch also in the implementation in
EAs, and the fact that heterogeneous, asynchronous
distributed architectures are now possible leads us to propose them as
an alternative to the single-language frameworks that are the most
usual nowadays. 

Future lines of work might include a more extensive measurement of
other operators such as  tournament selection and other selection
algorithms. A priori, these are essentially CPU integer
operations and their behavior might be, in principle, very similar to 
the one shown in these operations. This remains to be proved, however, but it is left
as future line of work. It would also be interesting to mix and match
different languages, choosing every one for its performance, in a hybrid
architecture. Communication might have some overhead, but it might be
offset by performance. Combining some compiled languages such as Go or
C with others characterized by its speed in some string operations,
like Perl or programming ease, like Python, might result in the best of both worlds:
performance and rapid prototyping. Creating a whole multi-language
framework along these lines is a challenge that might be interesting
in the future.

Focusing only in the part of measuring algorithms and in the interest
of reproductibility, we intend to create
a virtual machine image with all the tests so that it can be downloaded and
run anywhere. A comprehensive repository with tests and comments will
also be a good forum where different languages and implementations can
be discussed.  


\section*{Acknowledgements}

This paper is part of the open science effort at the university of
Granada. It has been written using {\tt knitr}, and its source as well as
the data used to create it can be downloaded from 
\href{https://github.com/JJ/2016-ea-languages-wcci}{the GitHub
  repository} \url{https://github.com/JJ/2016-ea-languages-wcci/}. 
It has been supported in part by
\href{http://geneura.wordpress.com}{GeNeura Team}, 
projects TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitiveness),
Conacyt Project PROINNOVA-220590,
PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza Centro de Estudios Andaluces 
en la IX Convocatoria de Proyectos de Investigaci{\'o}n), 
PROY-PP2015-06 (Plan Propio 2015 UGR), 
and project V17-2015 of the Microprojects program 2015 from CEI BioTIC Granada.  


\bibliographystyle{IEEEtran}
\bibliography{geneura,languages,GA-general}
%\bibliography{languages}
\end{document}


%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% hunspell-local-dictionary: "english"
%%% End:
